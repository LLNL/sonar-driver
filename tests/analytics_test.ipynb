{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Performance Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark and Sonar Cassandra Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonar_auth.cassandra import SonarCassandraSession\n",
    "session = SonarCassandraSession(['rzsonar8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonar_driver.spark import analytics as analytics\n",
    "from sonar_driver.spark import visuals as visuals\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-1.8.0/'\n",
    "os.environ['SPARK_HOME'] = '/g/g13/wang109/spark-2.3.1-bin-hadoop2.7'\n",
    "os.environ['JAVA_OPTS'] = '-Djavax.net.ssl.trustStore=/etc/pki/ca-trust/extracted/java/cacerts'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    '--master local[*] '\n",
    "    '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 '\n",
    "    'pyspark-shell'\n",
    ")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col, lit, split, udf, explode, asc, desc\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName('cassandra')\n",
    "        .config('spark.cassandra.connection.host', session.hosts_string)\n",
    "        .config('spark.cassandra.auth.username', session.username)\n",
    "        .config('spark.cassandra.auth.password', session.token)\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read job data from Cassandra and store in Spark dataframe with appropriate column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkdf = (\n",
    "    spark.read.format('org.apache.spark.sql.cassandra')\n",
    "        .options(keyspace='lcstaff_k', table='jobdata')\n",
    "        .load()\n",
    "        .select(['JobId', 'StartTime', 'scontrol'])\n",
    "        .withColumn('JobId', col('JobId').cast(IntegerType()))\n",
    "        .withColumn('StartTime', col('StartTime').cast(TimestampType()))\n",
    "        .withColumn('EndTime', col('scontrol')['EndTime'].cast(TimestampType()))\n",
    "        .withColumn('RunTime', col('scontrol')['RunTime'])\n",
    "        .withColumn('NodeList', col('scontrol')['NodeList'])\n",
    "        .drop('scontrol')\n",
    ")\n",
    "\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter jobs based on time range, nodes, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range = ['2018-05-16T07:27:21', '2018-05-17T07:27:21', 'EndTime']\n",
    "nodes = ['rztopaz', 'rzgenie36', 'rztronal[10-13]', 'rzalastor[10-15,20-24]']\n",
    "\n",
    "queried_sparkdf = analytics.query_jobs(sparkdf, time_range=time_range, nodes=nodes)\n",
    "queried_sparkdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate discrete derivatives based on window size and slide length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = analytics.discrete_derivatives(queried_sparkdf, 'EndTime', window_size=300, slide_length=300)\n",
    "derivatives.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot discrete derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.plot_derivatives(sparkdf=derivatives, column='EndTime', window_size=300, slide_length=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate discrete integrals based on slide length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrals = analytics.discrete_integrals(queried_sparkdf, slide_length=10)\n",
    "integrals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot discrete integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.plot_integrals(sparkdf=integrals, slide_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original dataframe is unaltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Allocations Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file of allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocs_file = 'allocs_file.txt'\n",
    "\n",
    "allocdf = (\n",
    "    spark.read\n",
    "        .format('com.databricks.spark.csv')\n",
    "        .option('delimiter', ',')\n",
    "        .option('header', 'false')\n",
    "        .load(allocs_file)\n",
    "        .toDF('address', 'size', 'alloc_time', 'free_time')\n",
    "        .withColumn('size', col('size').cast(DoubleType()).cast(IntegerType()))\n",
    "        .withColumn('alloc_time', col('alloc_time').cast(DoubleType()))\n",
    "        .withColumn('free_time', col('free_time').cast(DoubleType()))\n",
    ")\n",
    "\n",
    "max_free_time = allocdf.agg({\"free_time\": \"max\"}).collect()[0][0]\n",
    "set_free_time = udf(lambda t: max_free_time if t == 0 else t, DoubleType())\n",
    "allocdf = allocdf.withColumn('free_time', set_free_time('free_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics of unpooled and pooled allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max_memory_unpooled:', analytics.max_memory_unpooled(allocdf))\n",
    "print('max_memory_pooled:', analytics.max_memory_pooled(allocdf))\n",
    "print('total_bytesecs_unpooled:', analytics.total_bytesecs_unpooled(allocdf))\n",
    "print('total_bytesecs_pooled:', analytics.total_bytesecs_pooled(allocdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate minimum number of pools for each unique allocation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = sorted(analytics.pool_counts(allocdf), key=lambda p: -p['count'])\n",
    "pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot allocation pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.plot_allocs(sparkdf=allocdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIA",
   "language": "python",
   "name": "sia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
