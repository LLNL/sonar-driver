{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Performance Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark and Sonar Cassandra Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonar_driver.cassandra.session import SonarCassandraSession\n",
    "from sonar_driver.spark.session import SonarSparkSession\n",
    "\n",
    "from sonar_driver.spark import analytics\n",
    "from sonar_driver.spark import job_analytics\n",
    "from sonar_driver.spark import alloc_analytics\n",
    "from sonar_driver.spark import visuals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, lit, split, udf, explode, asc, desc\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from bokeh.layouts import column\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import CustomJS, ColumnDataSource, HoverTool, DatetimeTickFormatter, DataRange, Range, FactorRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Sonar nodes\n",
    "hosts = ['rzsonar8']\n",
    "\n",
    "# Path of Spark home\n",
    "spark_home = '/g/g13/wang109/spark-2.3.1-bin-hadoop2.7'\n",
    "\n",
    "scs = SonarCassandraSession(hosts=hosts)\n",
    "sss = (\n",
    "    SonarSparkSession(\n",
    "        spark_home=spark_home,\n",
    "        sonar_cassandra_session=scs\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read job data from Cassandra and store in Spark dataframe with appropriate column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for reading into Cassandra\n",
    "read_format = 'org.apache.spark.sql.cassandra'\n",
    "\n",
    "# Provide keyspace and table name\n",
    "keyspace, table = 'lcstaff_k', 'jobdata'\n",
    "\n",
    "# Reading into Cassandra and selecting columns for job ID's, start times, end times, runtimes, nodes.\n",
    "jobdf = (\n",
    "    sss.spark_session.read\n",
    "        .format(read_format)\n",
    "        .options(keyspace=keyspace, table=table)\n",
    "        .load()\n",
    "        .select(['JobId', 'StartTime', 'scontrol'])\n",
    "        .withColumn('JobId', col('JobId').cast(IntegerType()))\n",
    "        .withColumn('StartTime', col('StartTime').cast(TimestampType()))\n",
    "        .withColumn('EndTime', col('scontrol')['EndTime'].cast(TimestampType()))\n",
    "        .withColumn('RunTime', col('scontrol')['RunTime'])\n",
    "        .withColumn('NodeList', col('scontrol')['NodeList'])\n",
    "        .drop('scontrol')\n",
    ")\n",
    "\n",
    "jobdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query jobs based on time range, nodes, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your given schema must match the keys of this schema. \n",
    "# The values of a schema are the strings of the column names of the corresponding keys.\n",
    "schema = {\n",
    "    'name': 'jobdata',\n",
    "    'start': 'StartTime',\n",
    "    'end': 'EndTime',\n",
    "    'nodes': 'NodeList',\n",
    "    'users': 'User'\n",
    "}\n",
    "\n",
    "# Time range must follow the timestamp format below.\n",
    "time_range = ('2018-05-16T07:27:21', '2018-05-17T07:27:21')\n",
    "\n",
    "# List of nodes must follow the format in the given table.\n",
    "nodes = ['rztopaz', 'rzgenie36', 'rztronal[10-13]', 'rzalastor[10-15,20-24]']\n",
    "\n",
    "# Query jobs based on time range and nodes.\n",
    "queried_jobdf = job_analytics.query_jobs(jobdf, schema, time_range=time_range, nodes=nodes)\n",
    "queried_jobdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate discrete derivatives based on window size and slide length\n",
    "\n",
    "* 'column' refers to column (usually start times or end times) on which to calculate derivatives.\n",
    "* 'window_size' refers to dX.\n",
    "* 'slide_length' refers to the intervals at which derivatives are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = analytics.discrete_derivatives(sparkdf=queried_jobdf, column=schema['end'], \n",
    "                                             window_size=300, slide_length=300)\n",
    "derivatives.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot discrete derivatives\n",
    "* The 3 parameters after 'sparkdf' must match those passed into the 'discrete_derivatives' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.plot_derivatives(sparkdf=derivatives, column=schema['end'], window_size=300, slide_length=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate discrete integrals based on slide length\n",
    "* 'start_column' refers to start times column name.\n",
    "* 'end_column' refers to end times column name.\n",
    "* 'slide_length' refers to the intervals at which integrals are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrals = analytics.discrete_integrals(sparkdf=queried_jobdf, start_column=schema['start'], \n",
    "                                         end_column=schema['end'], slide_length=10)\n",
    "integrals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot discrete integrals\n",
    "* 'slide_length' must match the one passed into the 'discrete_derivatives' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.plot_integrals(sparkdf=integrals, slide_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Pandas dataframe for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf = queried_jobdf.sort('StartTime').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gantt chart of jobs unpooled\n",
    "* 'hist_var' refers to the column which contains the variable for the histogram.\n",
    "* 'hist_grouped' if True, each bin in the histogram will be a range of values; if False, each bin will be an individual value.\n",
    "* 'gantt_pooled' if True, resulting Gantt chart will pool objects; if False, each object will reside on its own horizontal line.\n",
    "* 'unit' refers to unit of timestamps in sparkdf.\n",
    "* 'df' is a Pandas dataframe which can be optionally inputed to reduce redundant operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visuals.plot_hist_gantt(queried_jobdf, start_column='StartTime', end_column='EndTime', df=qdf,\n",
    "                        hist_var='RunTime', hist_grouped=True, gantt_pooled=False, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gantt chart of jobs pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visuals.plot_hist_gantt(queried_jobdf, start_column='StartTime', end_column='EndTime', df=qdf,\n",
    "                        hist_var='RunTime', hist_grouped=True, gantt_pooled=True, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original dataframe is unaltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Allocations Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file of allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for reading txt file\n",
    "read_format = 'com.databricks.spark.csv'\n",
    "\n",
    "# File of allocations\n",
    "allocs_file = 'allocs_file.txt'\n",
    "\n",
    "# Reading into text and selecting columns for address, sizes, alloc times, free times.\n",
    "allocdf = (\n",
    "    sss.spark_session.read\n",
    "        .format(read_format)\n",
    "        .option('delimiter', ',')\n",
    "        .option('header', 'false')\n",
    "        .load(allocs_file)\n",
    "        .toDF('address', 'size', 'alloc_time', 'free_time')\n",
    "        .withColumn('size', col('size').cast(DoubleType()).cast(IntegerType()))\n",
    "        .withColumn('alloc_time', col('alloc_time').cast(DoubleType()))\n",
    "        .withColumn('free_time', col('free_time').cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Cleans allocation dataframe to account for allocs which were not deallocated.\n",
    "max_free_time = allocdf.agg({\"free_time\": \"max\"}).collect()[0][0]\n",
    "set_free_time = udf(lambda t: max_free_time if t == 0 else t, DoubleType())\n",
    "allocdf = allocdf.withColumn('free_time', set_free_time('free_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics of unpooled and pooled allocations\n",
    "* 'start_column' refers to start time column name\n",
    "* 'end_column' refers to end time column name\n",
    "* 'var' refers to variable on which to calculate metrics (size in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_column, end_column, var = 'alloc_time', 'free_time', 'size'\n",
    "\n",
    "max_memory_unpooled = alloc_analytics.max_memory_unpooled(allocdf, start_column, end_column, var)\n",
    "max_memory_pooled = alloc_analytics.max_memory_pooled(allocdf, start_column, end_column, var)\n",
    "total_bytesecs_unpooled = alloc_analytics.max_memory_pooled(allocdf, start_column, end_column, var)\n",
    "total_bytesecs_pooled = alloc_analytics.total_bytesecs_pooled(allocdf, start_column, end_column, var)\n",
    "\n",
    "print('max_memory_unpooled:', max_memory_unpooled)\n",
    "print('max_memory_pooled:', max_memory_pooled)\n",
    "print('total_bytesecs_unpooled:', total_bytesecs_unpooled)\n",
    "print('total_bytesecs_pooled:', total_bytesecs_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate minimum number of pools for each unique allocation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = sorted(alloc_analytics.pool_counts(allocdf, start_column, end_column, var), key=lambda p: -p['count'])\n",
    "pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Pandas dataframe for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = allocdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpooled Gantt chart of allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.plot_hist_gantt(allocdf, start_column='alloc_time', end_column='free_time', df=adf,\n",
    "                        hist_var='size', hist_grouped=False, gantt_pooled=False, unit='ns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled Gantt chart of allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.plot_hist_gantt(allocdf, start_column='alloc_time', end_column='free_time', df=adf,\n",
    "                        hist_var='size', hist_grouped=False, gantt_pooled=True, unit='ns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original dataframe is unaltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIA",
   "language": "python",
   "name": "sia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
