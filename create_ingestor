#!/usr/bin/env ipython

import sh
import os
import argparse
import json

from avro import schema


def avro_schema_from_file(filename):
    """ Read in an Avro file and generate the schema """

    with open(filename, 'r') as f:
        return schema.parse(f.read())


def parse_args():
    """ Parse arguments """

    parser = argparse.ArgumentParser(
            description="creates a Kafka ingestion file source for a Cassandra table, "
                        "creating the table if it does not yet exist.",
            formatter_class=argparse.RawTextHelpFormatter)

    parser.add_argument('-i', '--ingest-file', required=False,
            help="file to use as ingestion point, defaults to <table>.kafka")
    parser.add_argument('-p', '--primary_key', type=str, required=False,
            help="one or more primary keys, comma-separated, no spaces."
                "\nNOTE: this argument is required if Cassandra table is not yet created"
                "\nexamples:"
                "\n    primary_key"
                "\n    primary_key1,primary_key2")
    parser.add_argument('-c', '--cluster_key', type=str, required=False,
            help="one or more cluster keys, comma-separated, no spaces"
                 "\nexamples:"
                 "\n   cluster_key"
                 "\n   cluster_key1,cluster_key2")

    parser.add_argument('avro_schema_file', help="Avro schema file")
    parser.add_argument('keyspace', type=str, help="Cassandra keyspace to ingest into")
    parser.add_argument('table', type=str, help="Cassandra table to ingest into")

    return parser.parse_args()


def create_file_source(connector_name, topic_name, ingest_filename, avro_schema):
    """ Create a Cassandra sink connector configuration """

    return {
        "name" : connector_name,
        "config" : {
            "topic" : topic_name,
            "connector.class" : "FileStreamSourceConnector",
            "tasks.max" : "1",
            "file" : ingest_filename,
            "value.converter" : "io.confluent.connect.avro.AvroConverter",
            "value.converter.schema.registry.url" : "http://localhost:8081",
            "value.schema" : avro_schema.to_json()
        }
    }


def create_cassandra_sink(connector_name, topic_name, keyspace, table):
    """ Create a file source connector configuration """

    return {
        "name" : connector_name,
        "config" : {
            "topics" : topic_name,
            "connector.class" : "com.datamountaineer.streamreactor.connect.cassandra.sink.CassandraSinkConnector",
            "tasks.max" : "1",
            "connect.cassandra.kcql" : "INSERT INTO " + table + " SELECT * FROM " + topic_name,
            "connect.cassandra.contact.points" : "sonar11",
            "connect.cassandra.port" : "9042",
            "connect.cassandra.key.space" : keyspace,
            "connect.cassandra.username" : "cassandra",
            "connect.cassandra.password" : "cassandra"
        }
    }


def main():
    """ Main entrypoint """

    args = parse_args()

    avro_schema = avro_schema_from_file(args.avro_schema_file)

    keytable = args.keyspace + "-" + args.table
    connector_name = "avro-filesource-" + keytable
    topic_name = "avro-filesource-" + keytable
    ingest_file = args.ingest_file if args.ingest_file else keytable + ".kafka" 

    current_dir = os.getcwd()
    ingest_file_fullpath = os.path.join(current_dir, ingest_file)

    file_source = create_file_source(connector_name, topic_name, ingest_file_fullpath, avro_schema)
    cassandra_sink = create_cassandra_sink(connector_name, topic_name, args.keyspace, args.table)

    file_source_json = json.dumps(file_source, indent=4)
    cassandra_sink_json = json.dumps(cassandra_sink, indent=4)

    print(file_source_json)
    print(cassandra_sink_json)

if __name__ == '__main__':
    main()
